---
title: "Figure 5 - Maximum Likelihood Model and Genome-wide Estimate"
author: "Patrick Short"
date: "7 December 2016"
output: html_document
---

Code to generate Figure 4 of the non-coding burden paper.

```{r use power calc to generate ML model}

conserved_fb_active = read.table("../data/conserved_elements.min10_coverage.fb_active_roadmap_union.txt", header = TRUE, sep = "\t")

prevalence<-1/120 # fold enrichment of LOF mutations in DDD cohort
penetrance<-1 # penetrance of LoF mutations, to allow estimate of frequency of LoF mutations in cohort
num.trios<-6239
recurr=seq(2,6)
num.transmissions<-num.trios*2
thresh<-0.05/nrow(conserved_fb_active) # p value required to detect significantly mutated gene
max.DNMs<-10 # max number of DNMs to consider


n_lof = seq(0, 2613, 20)
lof.multiplier = seq(0.0001,.0801, 0.0001)

store_likelihood <-matrix(ncol=length(n_lof), nrow=length(lof.multiplier))
colnames(store_likelihood) = n_lof
rownames(store_likelihood) = lof.multiplier

p0 <- function(power) {
  power = power[sample(length(power), n_lof[i])]
  d = 1 - power # probability of NOT discovering each element
  prob_of_zero = prod(d) # probability of not discovering ANY of the elements 
  return(prob_of_zero)
}

for(i in seq(1,length(n_lof))) {

	for(j in seq(1,length(lof.multiplier))) {
		
	  lof.rate = conserved_fb_active$p_snp_null * lof.multiplier[j]
	  
	  # how many DNMs needed to meet significance threshold given this number of trios in this gene
		min.hits <- sapply(conserved_fb_active$p_snp_null, function(l) min(which(ppois(0:max.DNMs,l*num.trios*2, lower.tail=F)<thresh)))
		
		# how likely to see this minimum number of DNMs
		power <- ppois(min.hits-1, lof.rate*num.trios*2/prevalence*penetrance + conserved_fb_active$p_snp_null*num.trios*2, lower.tail=F)
		
		prob_of_zero = median(replicate(10, p0(power))) # probability of zero elements meeting genome-wide signifiance threshold
		
	  store_likelihood[j, i] = prob_of_zero 
		
	}
	
}	


library(reshape2)
library(ggplot2)
m = melt(store_likelihood, varnames = c("lof.rate", "n_lof"), value.name = "probability_of_finding_zero")
m$prop_lof = m$n_lof/max(m$n)

ggplot(m) + geom_tile(aes(lof.rate, n_lof, fill = probability_of_finding_zero)) + 
  scale_fill_gradient(low = "white", high = "blue", guide = guide_colorbar(title = "Probability of Observing\nZero Elements")) + 
  xlab("Proportion of Mutations\nResulting in Loss of Function") + 
  ylab("Number of Elements with\nMonoallelic Loss of Function Mechanism") +
  theme_bw(base_size = 18) +
  theme(strip.text = element_text(color="black"),strip.background = element_rect(fill="white", size=0),panel.border = element_blank()) + 
  theme(panel.grid.major = element_blank(),panel.grid.minor = element_blank())


ggplot(subset(m, (lof.rate < 0.04) & (n_lof < 1000))) + geom_tile(aes(lof.rate, n_lof, fill = probability_of_finding_zero)) + 
  scale_fill_gradient(low = "white", high = "blue", guide = guide_colorbar(title = "Probability of Observing\nZero Elements")) + 
  xlab("Proportion of Mutations\nResulting in Loss of Function") + 
  ylab("Number of Elements with\nMonoallelic Loss of Function Mechanism") +
  theme_bw(base_size = 18) +
  theme(strip.text = element_text(color="black"),strip.background = element_rect(fill="white", size=0),panel.border = element_blank()) + 
  theme(panel.grid.major = element_blank(),panel.grid.minor = element_blank())


```



Using the data we have here, we want to make a genome-wide estimate of the contribution of de novo SNVs.

We will consider all of the non-coding elements here and split into quantiles of phastcons100 score:

```{r load elements and score with phastcons}
library(phastCons100way.UCSC.hg19)
library(GenomicRanges)

conserved = read.table("../data/conserved_elements.min10_coverage.txt", header = TRUE, sep = "\t")
enhancers = read.table("../data/enhancer_elements.min10_coverage.txt", header = TRUE, sep = "\t")
heart = read.table("../data/heart_elements.min10_coverage.txt", header = TRUE, sep = "\t")
control = read.table("../data/noncoding_control_elements.10bp_buffer.min10_coverage.30bp_element_minimum.30x_probe_coverage_minimum.no_ddg2p_overlap.txt", header = TRUE, sep = "\t")
control = subset(control, (control$stop - control$start > 100))

all_elements = rbind(conserved[,-7], enhancers, heart, control)

gencode_v19_CDS = read.table("../data/gencode.v19.CDS.bed", header = FALSE, sep = "\t")
colnames(gencode_v19_CDS) = c("chr", "start", "stop", "chromHMM")


```

```{r load DHS data}
source("../R/mutation_null_model.R")
source("../R/annotation_tools.R")

male_fb_dhs = read.table(gzfile("../data/E081-DNase.hotspot.fdr0.01.broad.bed.gz"), header = FALSE, sep = "\t")
colnames(male_fb_dhs) = c("chr", "start", "stop", "id", "strength")
male_fb_chromHMM = read.table("../data/E081_15_coreMarks_mnemonics.bed", header = FALSE, sep = "\t")
colnames(male_fb_chromHMM) = c("chr", "start", "stop", "chromHMM")
male_fb_chromHMM = subset(male_fb_chromHMM, !(chromHMM %in% c("9_Het", "13_ReprPC", "14_ReprPCWk", "15_Quies")))

female_fb_dhs = read.table(gzfile("../data/E082-DNase.hotspot.fdr0.01.broad.bed.gz"), header = FALSE, sep = "\t")
colnames(female_fb_dhs) = c("chr", "start", "stop", "id", "strength")
female_fb_chromHMM = read.table("../data/E082_15_coreMarks_mnemonics.bed", header = FALSE, sep = "\t")
colnames(female_fb_chromHMM) = c("chr", "start", "stop", "chromHMM")
female_fb_chromHMM = subset(female_fb_chromHMM, !(chromHMM %in% c("9_Het", "13_ReprPC", "14_ReprPCWk", "15_Quies")))


# get the DDD specific fetal brain DHS

intersect_granges = function(b1, b2) {
  # intersect any two dataframes with chr, start, and stop
  b1 = GRanges(seqnames=Rle(b1$chr), ranges = IRanges(start = b1$start, end = b1$stop))
  b2 = GRanges(seqnames=Rle(b2$chr), ranges = IRanges(start = b2$start, end = b2$stop))
  i = intersect(b1, b2)
  
  new = data.frame(chr = as.character(i@seqnames),
                   start = as.integer(i@ranges@start),
                   stop = as.integer(i@ranges@start + i@ranges@width - 1))
  return(new)
}

difference_granges = function(b1, b2) {
  # take b1 and remove everything in b2
  b1 = GRanges(seqnames=Rle(b1$chr), ranges = IRanges(start = b1$start, end = b1$stop))
  b2 = GRanges(seqnames=Rle(b2$chr), ranges = IRanges(start = b2$start, end = b2$stop))
  i = setdiff(b1, b2)
  
  new = data.frame(chr = as.character(i@seqnames),
                   start = as.integer(i@ranges@start),
                   stop = as.integer(i@ranges@start + i@ranges@width - 1))
  return(new)
}

union_granges = function(b1, b2) {
  # intersect any two dataframes with chr, start, and stop
  b1 = GRanges(seqnames=Rle(b1$chr), ranges = IRanges(start = b1$start, end = b1$stop))
  b2 = GRanges(seqnames=Rle(b2$chr), ranges = IRanges(start = b2$start, end = b2$stop))
  i = union(b1, b2)
  
  new = data.frame(chr = as.character(i@seqnames),
                   start = as.integer(i@ranges@start),
                   stop = as.integer(i@ranges@start + i@ranges@width - 1))
  return(new)
}

#male_fb_chromHMM = subset(male_fb_chromHMM, !(chromHMM %in% c("9_Het", "15_Quies", "13_ReprPC", "14_ReprPCWk")))
#female_fb_chromHMM = subset(female_fb_chromHMM, !(chromHMM %in% c("9_Het", "15_Quies", "13_ReprPC", "14_ReprPCWk")))
#fb_dhs = union_granges(male_fb_chromHMM, female_fb_chromHMM)

fb_dhs = union_granges(male_fb_dhs, female_fb_dhs)
fb_dhs = difference_granges(fb_dhs, gencode_v19_CDS)

ddd_fb_dhs = intersect_granges(fb_dhs, all_elements)
ddd_fb_dhs$seq = as.character(get_sequence(ddd_fb_dhs$chr, ddd_fb_dhs$start, ddd_fb_dhs$stop))

ddd_fb_dhs$p_snp_null = 2 * sapply(ddd_fb_dhs$seq, p_sequence)

ddd_fb_dhs_intervals = GRanges(seqnames=ddd_fb_dhs$chr, IRanges(start = ddd_fb_dhs$start, width = ddd_fb_dhs$stop - ddd_fb_dhs$start + 1))
ddd_fb_dhs$phastcons100 = scores(phastCons100way.UCSC.hg19, ddd_fb_dhs_intervals)


hist(ddd_fb_dhs$phastcons100, xlab = "Evolutionary Conservation of DHS Peak (phastcons100)", main = "DDD phastcons distribution")

```

```{r plot observed v expected for each of the quantiles}

obs = read.table("../data/de_novos.ddd_8k.noncoding_included.2016-06-23.DHS_broad_peak_fdr_0.01.AllRoadmapTissues.txt", header = TRUE, sep = "\t")
obs = subset(obs, pp_dnm > 0.00781)
obs = subset(obs, nchar(as.character(ref)) == 1 & nchar(as.character(alt)) == 1)

blacklist = read.table("../data/8K_DF_Blacklist.txt", header = FALSE, sep = "\t")
blacklist_kinship = read.table("../data/8K_kinship_blacklist.txt", header = FALSE, sep = "\t")
blacklist = unique(c(as.character(blacklist$V1), as.character(blacklist_kinship$V1)))

obs = subset(obs, !(person_stable_id %in% blacklist))

diagnosed = read.table("../data/ddd_8k.diagnosed.2016-06-23.txt", header = TRUE, sep = "\t")
diagnosed = subset(diagnosed, !(person_id %in% blacklist))
diagnosed_sim_ids = seq(1, length(unique(diagnosed$person_id)))
obs_diagnosed = subset(obs, person_stable_id %in% diagnosed$person_id)
obs_undiagnosed = subset(obs, !(person_stable_id %in% diagnosed$person_id))

# load the file indicating whether proband has neurodev disorder (also used to get number of probands on blacklist)
has_neurodev = read.table("../data/ddd_8k_probands.neurodev_terms.txt", header = TRUE, sep = "\t")
has_neurodev$has_neurodev_phenotype = ifelse(has_neurodev$has_neurodev_phenotype == "True", TRUE, FALSE)
n_children_removed = sum(has_neurodev$person_stable_id %in% blacklist)
has_neurodev = subset(has_neurodev, !(person_stable_id %in% blacklist))
has_neurodev$diagnosed = has_neurodev$person_stable_id %in% diagnosed$person_id

undiagnosed_neurodev = has_neurodev$person_stable_id[has_neurodev$has_neurodev_phenotype & !has_neurodev$diagnosed]

o = subset(obs, person_stable_id %in% undiagnosed_neurodev)

library(plyr)

ddd_phastcons_q = unique(quantile(ddd_fb_dhs$phastcons100, seq(0, 1, length.out = 7)))
ddd_fb_dhs$phastcons_quantile = cut(ddd_fb_dhs$phastcons100, ddd_phastcons_q, include.lowest = TRUE)
ddd = ddply(ddd_fb_dhs, "phastcons_quantile", function(df) data.frame(expected = sum(df$p_snp_null)*length(undiagnosed_neurodev), observed = nrow(filter_with_bed(o, df))))
ddd$mid = (ddd_phastcons_q[1:(length(ddd_phastcons_q)-1)] + ddd_phastcons_q[2:(length(ddd_phastcons_q))])/2
ddd$ratio = ddd$observed/ddd$expected


```

Sliding window approach:

```{r sliding window approach}

obs_expected_df <- function(phastcons_low, phastcons_high, obs, elements) {
  elements = subset(elements, (phastcons100 >= phastcons_low) & (phastcons100 <= phastcons_high))
  return(data.frame(phastcons_low = phastcons_low, phastcons_high = phastcons_high, mid = (phastcons_low + phastcons_high)/2, expected = sum(elements$p_snp_null)*length(undiagnosed_neurodev), observed = nrow(filter_with_bed(obs, elements))))
}

obs_expected_df <- function(a, b, obs, elements) {
  elements = elements[order(elements$phastcons100),]
  elements = elements[a:b,]
  return(data.frame(mid = median(elements$phastcons100), expected = sum(elements$p_snp_null)*length(undiagnosed_neurodev), observed = nrow(filter_with_bed(obs, elements))))
}

n = 1000
j = 100
starts = seq(1, nrow(ddd_fb_dhs) - n - 1, by = j)
stops = seq(n, nrow(ddd_fb_dhs), by = j)

ddd = do.call(rbind,mapply(obs_expected_df, starts, stops, MoreArgs = list(obs = o, elements = ddd_fb_dhs), SIMPLIFY = FALSE))

#ddd = do.call(rbind,mapply(obs_expected_df, seq(0,0.7, 0.01), seq(0.3, 1, 0.01), MoreArgs = list(obs = o, elements = ddd_fb_dhs), SIMPLIFY = FALSE))


ddd$ratio = ddd$observed/ddd$expected

```

Now, estimate the genome-wide mutability in each of these categories.

```{r genome-wide mutability}

nc = read.table("../data/phastcons_g10_g100bp.bed", header = FALSE, sep = "\t")
colnames(nc) = c("chr", "start", "stop", "lod", "score")

set.seed(42)  # we pick 1000 randomly sampled sequences in because generating phastcons and mutability is slow
sample_size = 1000
#fb_dhs_nc = intersect_granges(fb_dhs, nc)
#fb_dhs_nc$bp = fb_dhs_nc$stop - fb_dhs_nc$start
#fb_dhs_nc = subset(fb_dhs_nc, bp > 100)
#multiplier = nrow(fb_dhs_nc)/sample_size
#fb_dhs_sample = fb_dhs_nc[sample(seq(1, nrow(fb_dhs_nc)), sample_size, replace = FALSE),]

multiplier = nrow(fb_dhs)/sample_size
fb_dhs_sample = fb_dhs[sample(seq(1, nrow(fb_dhs)), sample_size, replace = FALSE),]

fb_dhs_sample$seq = as.character(get_sequence(fb_dhs_sample$chr, fb_dhs_sample$start, fb_dhs_sample$stop))
fb_dhs_sample$p_snp_null = 2 * sapply(fb_dhs_sample$seq, p_sequence)

wg = GRanges(seqnames=fb_dhs_sample$chr, IRanges(start = fb_dhs_sample$start, width = fb_dhs_sample$stop - fb_dhs_sample$start + 1))
fb_dhs_sample$phastcons100 = scores(phastCons100way.UCSC.hg19, wg)

fb_dhs_sample$phastcons_quantile = cut(fb_dhs_sample$phastcons100, ddd_phastcons_q, include.lowest = TRUE)

wg = ddply(fb_dhs_sample, "phastcons_quantile", function(df) data.frame(expected = sum(df$p_snp_null)*length(undiagnosed_neurodev)*multiplier))

hist(fb_dhs_sample$phastcons100, xlab = "Evolutionary Conservation of DHS Peak (phastcons100)", main = "Whole Genome phastcons distribution")


# plot distribution in each bin
fb_dhs_sample$phastcons_bins = cut(fb_dhs_sample$phastcons100, seq(0,1,0.1), include.lowest = TRUE)
megabase = ddply(fb_dhs_sample, "phastcons_bins", function(df) data.frame(megabase = sum(df$stop - df$start)*multiplier/1e6))

barplot(megabase$megabase)
```


Using logistic regression:

```{r logit estimate}
d = ddd
d$excess = d$ratio - 1
d$excess[d$excess < 0] = 0
#d$excess[d$mid < break_point] = 0

#d$excess = d$excess
#d = subset(d, mid > 0.8)

fit <- glm(excess/max(d$excess) ~ mid, data=d, family=binomial())
fit_pred = predict(fit, data.frame(mid = seq(0,1,0.01)), type = "response", se.fit = TRUE)
fit_lower <- approxfun(seq(0,1,0.01), 1 + (fit_pred$fit - 1.96*fit_pred$se.fit)*max(d$excess))
fit_upper <- approxfun(seq(0,1,0.01), 1 + (fit_pred$fit + 1.96*fit_pred$se.fit)*max(d$excess))

plot(d$mid, d$ratio, xlim = c(0, 1.0), ylim = c(0.5, 1.5), ylab = "Observed/Expected in DDD", xlab = "Evolutionary Conservation of DHS peak (phastcons100)", main = "DNM enrichment across evolutionary conservation spectrum\n")
#for (i in seq(1,nrow(d))) {
  #lines(x = c(d$mid[i], d$mid[i]), y = c(d$observed[i]/(d$expected[i] - 2*sqrt(d$expected[i])), d$observed[i]/(d$expected[i] + 2*sqrt(d$expected[i]))))
#}
fit_plot = predict(fit, data.frame(mid = seq(0,1,0.01)), type = "response", se.fit = TRUE)
lines(seq(0,1,0.01), 1 + fit_plot$fit*max(d$excess))
lines(seq(0,1,0.01), 1 + (fit_plot$fit + 1.96*fit_plot$se.fit)*max(d$excess))
lines(seq(0,1,0.01), 1 + (fit_plot$fit - 1.96*fit_plot$se.fit)*max(d$excess))
abline(h = 1.0)

fb_dhs_sample$predicted_excess = predict(fit, data.frame(mid = fb_dhs_sample$phastcons100), type = "response")*max(d$excess)
fb_dhs_sample$predicted_excess[fb_dhs_sample$predicted_excess < 0] = 0

fb_dhs_sample$predicted_excess_lower = fit_lower(fb_dhs_sample$phastcons100) - 1
fb_dhs_sample$predicted_excess_lower[fb_dhs_sample$predicted_excess_lower < 0] = 0

fb_dhs_sample$predicted_excess_upper = fit_upper(fb_dhs_sample$phastcons100) - 1
fb_dhs_sample$predicted_excess_upper[fb_dhs_sample$predicted_excess_upper < 0] = 0

total_predicted = sum(fb_dhs_sample$p_snp_null*length(undiagnosed_neurodev)*fb_dhs_sample$predicted_excess)*multiplier
total_predicted_lower = sum(fb_dhs_sample$p_snp_null*length(undiagnosed_neurodev)*fb_dhs_sample$predicted_excess_lower)*multiplier
total_predicted_upper = sum(fb_dhs_sample$p_snp_null*length(undiagnosed_neurodev)*fb_dhs_sample$predicted_excess_upper)*multiplier

```
